# Human vs Machine Learning Project

This project challenges you to explore the differences between human-designed algorithms and machine learning models. You will first create a human algorithm (pseudo-code) to classify data based on features, then translate that algorithm into Python. Next, you will train a K-Nearest Neighbors (KNN) classifier on the same dataset and compare your results. Finally, you will record a short screen-share with narration explaining your methods and observations.

You may work alone or with a partner. You may choose to work with the provided Penguins dataset, or select your own pre-cleaned dataset from the links below (I have suggested a few datasets as a guide, but you are welcome to select something different with approval).  The most important detail regarding your data-set is that your data needs to lend itself to classification.  For example, an iris with a sepal length of x and a petal width of y can be classified as ‘Setosa’. I also recommend that you use github codespaces, as you will need access to command-line tools that are unavailable in VS Code for EDU.

[UCI Machine Learning Repository](https://archive.ics.uci.edu/datasets)
 - Iris (classic 3-class classification)
 - Mushroom (binary classification: edible/poisonous)
 - Student Performance (predict grades, numeric features)

[Kaggle Datasets](https://www.kaggle.com/datasets)   *Note: For Kaggle, I will have to download the data for you and post on a shared drive.
 - Titanic survival dataset (binary classification)
 - Heart disease dataset (binary classification)
 - Breast cancer diagnosis (binary)
 - Penguins dataset (same as Kira, already cleaned)

---

**Team Members:**  
- Name 1  : Kaitlyn Murray
- Name 2 : Ella Dalton  

**Dataset Used:**  
Student Peformance

**Source:**  
UCI

**Target Variable (What we are predicting):**  
We are predicting whether or not a student is passing or failing.

**Features Used:**  
- Studytime (measurement in hours on a scale of 1-4)*
- Family Relationship (measurement based on a scale of 1-5, 1 very bad and 5 excellent)
- Feature 3

**[Video Review](https://)**

## Human Algorithm

### Pseudo-Code
```text
The best difference that was most visiable to us was when studytime is compared over different students, so the human algorithm is:

    IF studytime >= 2:
        Predict "Passing"
    ELSE:
        Predict "Failing
```

When examining the data and visualizations, we focused on the feature studytime because it showed the more accurate results compared to when family relationships was used.

The plots/tables suggested a possible threshold for studytime, and we considered values above or below 2 to see how they might relate to students passing or failing.

From the summary tables and visualizations, it appeared that family relationships could influence classification, which led us to use this factor in our decision rules.

### Confusion Matrix

Accuracy: 64.10%

| Actual \ Predicted | Failing | Passing | Class 3 |
|-------------------|---------|---------|---------|
| **Failing**       |    42     |    48     |         |
| **Passing**       |    22     |    83     |         |
| **Class 3**       |         |         |         |

One example where our algorithm worked well is when the inputs were ___, leading to a correct prediction of ___ because ___.

An example where the algorithm did not perform as expected is when the inputs were ___, resulting in a prediction of ___ instead of ___, which may have happened because ___.

These examples of success and failure highlight patterns in the data or limitations in our rules, such as ___.

<img width="315" height="334" alt="image" src="https://github.com/user-attachments/assets/23ee1e49-da76-47c2-97b8-c8fbcbef179c" />

## Machine Learning Model

We chose a value of k = 65 after comparing model performance across different values of k and observing that this was the lowest amount we could do that had the highest accuracy.

When analyzing the outputs and metrics, we noticed that changing k affected the accuracy, which influenced our final choice.

Based on the results shown in the tables or visualizations, k = 65 best matched our goals for model performance because of how much data we had and the small range we had to reprsent our data.

### Confusion Matrix

Accuracy: 65.64%

| Actual \ Predicted | Failing | Passing | Class 3 |
|-------------------|---------|---------|---------|
| **Failing**       |   56      |    34     |         |
| **Passing**       |   33     |     72    |         |
| **Class 3**       |         |         |         |

The table/visualization shows a clear pattern where the model predicts ___ when ___, indicating a strong relationship between these features.

The confusion matrix reveals that the model most often confuses failing with passing, suggesting these classes have similar feature values.

Compared to the human algorithm, the KNN model shows different behavior when ___, as seen in the ___ visualization.

<img width="315" height="334" alt="image" src="https://github.com/user-attachments/assets/199ae59d-3470-40c6-9669-60e62b211619" />
